{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fZ9Vjp4xiQNs",
    "outputId": "377276f8-0ea6-4393-93d0-d915a4b5547d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Student\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTT program starts...\n",
      "The 4 page https://www.ptt.cc/bbs/Food/index6997.html starts:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Student\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The page 4 20 https://www.ptt.cc/bbs/Food/M.1561007493.A.CDE.html\n",
      "The page 4 19 https://www.ptt.cc/bbs/Food/M.1561014236.A.616.html\n",
      "The page 4 18 https://www.ptt.cc/bbs/Food/M.1561018015.A.BC6.html\n",
      "The page 4 17 https://www.ptt.cc/bbs/Food/M.1561021014.A.A71.html\n",
      "The page 4 16 https://www.ptt.cc/bbs/Food/M.1561025307.A.673.html\n",
      "The page 4 15 https://www.ptt.cc/bbs/Food/M.1561029185.A.ADE.html\n",
      "The page 4 14 https://www.ptt.cc/bbs/Food/M.1561029260.A.244.html\n",
      "The page 4 13 https://www.ptt.cc/bbs/Food/M.1561029412.A.F17.html\n",
      "The page 4 12 https://www.ptt.cc/bbs/Food/M.1561029728.A.435.html\n",
      "The page 4 11 https://www.ptt.cc/bbs/Food/M.1561030638.A.E31.html\n",
      "The page 4 10 https://www.ptt.cc/bbs/Food/M.1561036822.A.C8B.html\n",
      "The page 4 9 https://www.ptt.cc/bbs/Food/M.1561039829.A.524.html\n",
      "The page 4 8 https://www.ptt.cc/bbs/Food/M.1561040069.A.3A9.html\n",
      "The page 4 7 https://www.ptt.cc/bbs/Food/M.1561040232.A.D16.html\n",
      "The page 4 6 https://www.ptt.cc/bbs/Food/M.1561040410.A.C26.html\n",
      "The page 4 5 https://www.ptt.cc/bbs/Food/M.1561041784.A.AB2.html\n",
      "The page 4 4 https://www.ptt.cc/bbs/Food/M.1561042049.A.6D3.html\n",
      "The page 4 3 https://www.ptt.cc/bbs/Food/M.1561042331.A.436.html\n",
      "The page 4 2 https://www.ptt.cc/bbs/Food/M.1561042948.A.C18.html\n",
      "The page 4 1 https://www.ptt.cc/bbs/Food/M.1561043932.A.BE5.html\n",
      "The 3 page https://www.ptt.cc/bbs/Food/index6998.html starts:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Student\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The page 3 20 https://www.ptt.cc/bbs/Food/M.1561044272.A.75C.html\n",
      "The page 3 19 https://www.ptt.cc/bbs/Food/M.1561045507.A.546.html\n",
      "The page 3 18 https://www.ptt.cc/bbs/Food/M.1561046219.A.FDC.html\n",
      "The page 3 17 https://www.ptt.cc/bbs/Food/M.1561047034.A.EBF.html\n",
      "The page 3 16 https://www.ptt.cc/bbs/Food/M.1561048912.A.4C5.html\n",
      "The page 3 15 https://www.ptt.cc/bbs/Food/M.1561061794.A.0A1.html\n",
      "The page 3 14 https://www.ptt.cc/bbs/Food/M.1561077422.A.75F.html\n",
      "The page 3 13 https://www.ptt.cc/bbs/Food/M.1561081769.A.36A.html\n",
      "The page 3 12 https://www.ptt.cc/bbs/Food/M.1561084986.A.ACE.html\n",
      "The page 3 11 https://www.ptt.cc/bbs/Food/M.1561087645.A.60A.html\n",
      "The page 3 10 https://www.ptt.cc/bbs/Food/M.1561091545.A.E27.html\n",
      "The page 3 9 https://www.ptt.cc/bbs/Food/M.1561091591.A.640.html\n",
      "The page 3 8 https://www.ptt.cc/bbs/Food/M.1561092942.A.004.html\n",
      "The page 3 7 https://www.ptt.cc/bbs/Food/M.1561094623.A.449.html\n",
      "The page 3 6 https://www.ptt.cc/bbs/Food/M.1561096881.A.498.html\n",
      "The page 3 5 https://www.ptt.cc/bbs/Food/M.1561102294.A.E02.html\n",
      "The page 3 4 https://www.ptt.cc/bbs/Food/M.1561105873.A.F3A.html\n",
      "The page 3 3 https://www.ptt.cc/bbs/Food/M.1561112472.A.F06.html\n",
      "The page 3 2 https://www.ptt.cc/bbs/Food/M.1561117273.A.C32.html\n",
      "The page 3 1 https://www.ptt.cc/bbs/Food/M.1561119541.A.9F3.html\n",
      "The 2 page https://www.ptt.cc/bbs/Food/index6999.html starts:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Student\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The page 2 20 https://www.ptt.cc/bbs/Food/M.1561121122.A.F60.html\n",
      "The page 2 19 https://www.ptt.cc/bbs/Food/M.1561121332.A.D2D.html\n",
      "The page 2 18 https://www.ptt.cc/bbs/Food/M.1561122542.A.32B.html\n",
      "The page 2 17 https://www.ptt.cc/bbs/Food/M.1561124570.A.D48.html\n",
      "The page 2 16 https://www.ptt.cc/bbs/Food/M.1561125186.A.E00.html\n",
      "The page 2 15 https://www.ptt.cc/bbs/Food/M.1561126190.A.84B.html\n",
      "The page 2 14 https://www.ptt.cc/bbs/Food/M.1561129759.A.F68.html\n",
      "The page 2 13 https://www.ptt.cc/bbs/Food/M.1561130157.A.2B0.html\n",
      "The page 2 12 https://www.ptt.cc/bbs/Food/M.1561130256.A.752.html\n",
      "The page 2 11 https://www.ptt.cc/bbs/Food/M.1561132196.A.A9D.html\n",
      "The page 2 10 https://www.ptt.cc/bbs/Food/M.1561132739.A.5E3.html\n",
      "The page 2 9 https://www.ptt.cc/bbs/Food/M.1561138634.A.36C.html\n",
      "The page 2 8 https://www.ptt.cc/bbs/Food/M.1561157401.A.CD4.html\n",
      "The page 2 7 https://www.ptt.cc/bbs/Food/M.1561171484.A.CEA.html\n",
      "The page 2 6 https://www.ptt.cc/bbs/Food/M.1561171756.A.B87.html\n",
      "The page 2 5 https://www.ptt.cc/bbs/Food/M.1561177469.A.44D.html\n",
      "The page 2 4 https://www.ptt.cc/bbs/Food/M.1561178251.A.56D.html\n",
      "The page 2 3 https://www.ptt.cc/bbs/Food/M.1561178260.A.766.html\n",
      "The page 2 2 https://www.ptt.cc/bbs/Food/M.1561183175.A.628.html\n",
      "The page 2 1 https://www.ptt.cc/bbs/Food/M.1561189840.A.6FD.html\n",
      "The 1 page https://www.ptt.cc/bbs/Food/index7000.html starts:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Student\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The page 1 21 https://www.ptt.cc/bbs/Food/M.1561192853.A.641.html\n",
      "The page 1 20 https://www.ptt.cc/bbs/Food/M.1561203873.A.E4B.html\n",
      "The page 1 19 https://www.ptt.cc/bbs/Food/M.1561204725.A.C84.html\n",
      "The page 1 18 https://www.ptt.cc/bbs/Food/M.1561205609.A.A36.html\n",
      "The page 1 17 https://www.ptt.cc/bbs/Food/M.1561206651.A.15D.html\n",
      "The page 1 16 https://www.ptt.cc/bbs/Food/M.1561208681.A.B19.html\n",
      "The page 1 15 https://www.ptt.cc/bbs/Food/M.1561210361.A.61D.html\n",
      "The page 1 14 https://www.ptt.cc/bbs/Food/M.1561212533.A.239.html\n",
      "The page 1 13 https://www.ptt.cc/bbs/Food/M.1561216093.A.0A7.html\n",
      "The page 1 12 https://www.ptt.cc/bbs/Food/M.1561217677.A.CC6.html\n",
      "The page 1 11 https://www.ptt.cc/bbs/Food/M.1561217678.A.6C3.html\n",
      "The page 1 10 https://www.ptt.cc/bbs/Food/M.1561218832.A.7C1.html\n",
      "The page 1 9 https://www.ptt.cc/bbs/Food/M.1561233505.A.259.html\n",
      "The page 1 8 https://www.ptt.cc/bbs/Food/M.1561247533.A.833.html\n",
      "The page 1 7 https://www.ptt.cc/bbs/Food/M.1561253460.A.AB1.html\n",
      "The page 1 6 https://www.ptt.cc/bbs/Food/M.1561255693.A.1F6.html\n",
      "The page 1 5 https://www.ptt.cc/bbs/Food/M.1561260894.A.6B4.html\n",
      "The page 1 4 https://www.ptt.cc/bbs/Food/M.1355673582.A.5F7.html\n",
      "The page 1 3 https://www.ptt.cc/bbs/Food/M.1190944426.A.E6C.html\n",
      "The page 1 2 https://www.ptt.cc/bbs/Food/M.1128132666.A.0FD.html\n",
      "The page 1 1 https://www.ptt.cc/bbs/Food/M.1496532469.A.C36.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Student\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:168: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert IP into city name. Starts...\n",
      "77 ...\n",
      "76 ...\n",
      "75 ...\n",
      "74 ...\n",
      "73 ...\n",
      "72 ...\n",
      "71 ...\n",
      "70 ...\n",
      "69 ...\n",
      "68 ...\n",
      "67 ...\n",
      "66 ...\n",
      "65 ...\n",
      "64 ...\n",
      "63 ...\n",
      "62 ...\n",
      "61 ...\n",
      "60 ...\n",
      "59 ...\n",
      "58 ...\n",
      "57 ...\n",
      "56 ...\n",
      "55 ...\n",
      "54 ...\n",
      "53 ...\n",
      "52 ...\n",
      "51 ...\n",
      "50 ...\n",
      "49 ...\n",
      "48 ...\n",
      "47 ...\n",
      "46 ...\n",
      "45 ...\n",
      "44 ...\n",
      "43 ...\n",
      "42 ...\n",
      "41 ...\n",
      "40 ...\n",
      "39 ...\n",
      "38 ...\n",
      "37 ...\n",
      "36 ...\n",
      "35 ...\n",
      "34 ...\n",
      "33 ...\n",
      "32 ...\n",
      "31 ...\n",
      "30 ...\n",
      "29 ...\n",
      "28 ...\n",
      "27 ...\n",
      "26 ...\n",
      "25 ...\n",
      "24 ...\n",
      "23 ...\n",
      "22 ...\n",
      "21 ...\n",
      "20 ...\n",
      "19 ...\n",
      "18 ...\n",
      "17 ...\n",
      "16 ...\n",
      "15 ...\n",
      "14 ...\n",
      "13 ...\n",
      "12 ...\n",
      "11 ...\n",
      "10 ...\n",
      "9 ...\n",
      "8 ...\n",
      "7 ...\n",
      "6 ...\n",
      "5 ...\n",
      "4 ...\n",
      "3 ...\n",
      "2 ...\n",
      "1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Student\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:201: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Student\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:228: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Student\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:239: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Student\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Student\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:250: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Student\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:255: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Student\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:260: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Student\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:6586: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n",
      "C:\\Users\\Student\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:290: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : Inserted into the database.\n",
      "1 : Inserted into the database.\n",
      "2 : Inserted into the database.\n",
      "3 : Inserted into the database.\n",
      "4 : Inserted into the database.\n",
      "5 : Inserted into the database.\n",
      "6 : Inserted into the database.\n",
      "7 : Inserted into the database.\n",
      "8 : Inserted into the database.\n",
      "9 : Inserted into the database.\n",
      "10 : Inserted into the database.\n",
      "11 : Inserted into the database.\n",
      "12 : Inserted into the database.\n",
      "13 : Inserted into the database.\n",
      "14 : Inserted into the database.\n",
      "15 : Inserted into the database.\n",
      "16 : Inserted into the database.\n",
      "17 : Inserted into the database.\n",
      "18 : Inserted into the database.\n",
      "19 : Inserted into the database.\n",
      "20 : Inserted into the database.\n",
      "21 : Inserted into the database.\n",
      "22 : Inserted into the database.\n",
      "23 : Inserted into the database.\n",
      "24 : Inserted into the database.\n",
      "25 : Inserted into the database.\n",
      "26 : Inserted into the database.\n",
      "27 : Inserted into the database.\n",
      "28 : Inserted into the database.\n",
      "29 : Inserted into the database.\n",
      "30 : Inserted into the database.\n",
      "31 : Inserted into the database.\n",
      "32 : Inserted into the database.\n",
      "33 : Inserted into the database.\n",
      "34 : Inserted into the database.\n",
      "35 : Inserted into the database.\n",
      "36 : Inserted into the database.\n",
      "37 : Inserted into the database.\n",
      "38 : Inserted into the database.\n",
      "39 : Inserted into the database.\n",
      "40 : Updated the database.\n",
      "41 : Updated the database.\n",
      "42 : Updated the database.\n",
      "43 : Updated the database.\n",
      "44 : Updated the database.\n",
      "45 : Updated the database.\n",
      "46 : Updated the database.\n",
      "47 : Updated the database.\n",
      "48 : Updated the database.\n",
      "49 : Updated the database.\n",
      "50 : Updated the database.\n",
      "51 : Updated the database.\n",
      "52 : Updated the database.\n",
      "53 : Updated the database.\n",
      "54 : Updated the database.\n",
      "55 : Updated the database.\n",
      "56 : Updated the database.\n",
      "57 : Updated the database.\n",
      "58 : Updated the database.\n",
      "59 : Updated the database.\n",
      "60 : Updated the database.\n",
      "61 : Updated the database.\n",
      "62 : Updated the database.\n",
      "63 : Updated the database.\n",
      "64 : Updated the database.\n",
      "65 : Updated the database.\n",
      "66 : Updated the database.\n",
      "67 : Updated the database.\n",
      "68 : Updated the database.\n",
      "69 : Updated the database.\n",
      "70 : Updated the database.\n",
      "71 : Updated the database.\n",
      "72 : Updated the database.\n",
      "73 : Updated the database.\n",
      "74 : Updated the database.\n",
      "75 : Updated the database.\n",
      "76 : Updated the database.\n",
      "Finished\n",
      "Total time: 2 m 11 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup \n",
    "from selenium import webdriver\n",
    "\n",
    "\n",
    "# # The following method catched all the PTT Food pages' URL from year 2004 to today\n",
    "# urls = ['https://www.ptt.cc/bbs/Food/index{}.html'.format(str(i)) for i in range(7000,7003)]\n",
    "# urls = urls[::-1]\n",
    "# url_list = urls[1:2] # <-- Change the range to assign the pages to crawl\n",
    "\n",
    "res = requests.get('https://www.ptt.cc/bbs/Food/index.html', verify=False)\n",
    "soup = BeautifulSoup(res.text,'html.parser')\n",
    "\n",
    "page_url_list = []\n",
    "\n",
    "#取得上一頁的ptt_page_url\n",
    "page_url = soup.select('a.btn.wide')[1]\n",
    "ptt_page_url = 'https://www.ptt.cc'+page_url.get('href')\n",
    "res2 = requests.get(ptt_page_url)\n",
    "new_url =BeautifulSoup(res2.text,'html.parser')\n",
    "\n",
    "#將上一頁的ptt_page_url的index分割\n",
    "page_now = ptt_page_url.split('index')[1].replace(\".html\",\"\")\n",
    "page_number = int(page_now)\n",
    "page_number = page_number + 2\n",
    "for i in range(page_number):\n",
    "    web_url = f'https://www.ptt.cc/bbs/Food/index{i}.html'\n",
    "    page_url_list.append(web_url)\n",
    "\n",
    "#取得最後2頁的ptt_page_url (可變)\n",
    "url_list = page_url_list[-4:] # <= change here!!!\n",
    "\n",
    "start = time.time()\n",
    "print('PTT program starts...')\n",
    "# Get all the articles' URL in the given page\n",
    "content_list = []\n",
    "comments_list = []\n",
    "except_list = []\n",
    "\n",
    "page = len(url_list)\n",
    "for url in url_list:\n",
    "    print('The', page, 'page', url,'starts:')\n",
    "    #page -= 1\n",
    "    res = requests.get(url, verify=False)\n",
    "#     time.sleep(2)\n",
    "    html_str = res.text\n",
    "    soup = BeautifulSoup(html_str)\n",
    "\n",
    "    title_list = []\n",
    "\n",
    "    for i in soup.select('.title a'):\n",
    "        route = 'https://www.ptt.cc' + i.get('href')\n",
    "        title_list.append(route)\n",
    "    # print(title_list)\n",
    "    num = len(title_list)\n",
    "    # Finally, we are able to get the contents\n",
    "    for url2 in title_list:\n",
    "        print('The page',page,num, url2)\n",
    "        num -= 1\n",
    "#         time.sleep(2)\n",
    "        r2 = requests.get(url2)\n",
    "        soup = BeautifulSoup(r2.text, 'html')\n",
    "        main_content = soup.select('#main-content')\n",
    "\n",
    "        info = soup.select('.article-metaline')\n",
    "        # The main content is the whole thing except the article's info\n",
    "        info = [i.text for i in info]\n",
    "        # Get the title\n",
    "        if info != []:\n",
    "            try:\n",
    "                title = info[1]\n",
    "                # Get the time\n",
    "                date = info[2].replace('時間', '')\n",
    "                info.insert(1, '看板Food')\n",
    "                info_str = ''.join(info)\n",
    "                # Clean the title, author, and other info\n",
    "                main_content = main_content[0].text.replace(info_str, '')\n",
    "\n",
    "                # Split the whole thing into the main content and the comments sections\n",
    "                main_content = main_content.split('※ 發信站: 批踢踢實業坊')\n",
    "                content = main_content[0]  # This is the main content\n",
    "                # Start clean the content\n",
    "                # Clean out '\\n'\n",
    "                content = content.replace('\\n', '')  \n",
    "                #clean url\n",
    "                url_pattern = r'((http|ftp|https):\\/\\/)?[\\w\\-_]+(\\.[\\w\\-_]+)+([\\w\\-\\.,@?^=%&amp;:/~\\+#]*[\\w\\-\\@?^=%&amp;/~\\+#])?'\n",
    "                text = re.sub(url_pattern, ' ', content)\n",
    "                #clean all marks\n",
    "                ptt_content = re.sub('[^\\u4e00-\\u9fa5\\u0030-\\u0039\\u0041-\\u005a\\u0061-\\u007a]','',text)\n",
    "                #clean XD\n",
    "                content = re.sub('XDD*','',ptt_content) \n",
    "            \n",
    "                if len(main_content) >= 2:\n",
    "                    \n",
    "                    comments = main_content[1]  # This section is all the comments\n",
    "                    comments = comments.split('.html')\n",
    "                    # Get the IP address\n",
    "                    ip_addr = comments[0].split('※ 文章網址')\n",
    "                    ip_addr = ip_addr[0].split('來自: ')\n",
    "                    ip_addr = ip_addr[1]\n",
    "                    ip_addr = ip_addr.replace('\\n', '')\n",
    "                    comments = comments[1]\n",
    "                    comments = comments.split('\\n')\n",
    "                    \n",
    "                    # Count pushes and boos, and append all comments\n",
    "                    push = 0\n",
    "                    boo = 0\n",
    "                    comment_count = 0\n",
    "                    all_comments = ''\n",
    "                    for i in comments:\n",
    "                        if len(i) != 0:\n",
    "                            if i[0] == '推':\n",
    "                                push += 1\n",
    "                            elif i[0] == '噓':\n",
    "                                boo += 1\n",
    "                            elif i[0] == '→':\n",
    "                                comment_count += 1\n",
    "                            i = i.split(' ')\n",
    "                            if i[0] != '※':\n",
    "                                i = i[2:-2]\n",
    "                                i = ''.join(i)\n",
    "                                all_comments += i\n",
    "\n",
    "                    total_comment_count = comment_count + push + boo\n",
    "\n",
    "                    content_list.append(\n",
    "                        {'url': url2, 'title': title, 'time': date, 'ip': ip_addr, 'content': content, 'push': push,\n",
    "                         'boo': boo, 'total': total_comment_count})\n",
    "                    comments_list.append({'url': url2, 'comment': all_comments})\n",
    "            except:\n",
    "                except_list.append(url2)    \n",
    "        else:\n",
    "            except_list.append(url2)\n",
    "    page -= 1\n",
    "\n",
    "\n",
    "# Automatic naming the file \n",
    "today = str(datetime.date.today())\n",
    "today = today.replace('-','_')\n",
    "\n",
    "with open('ptt_content{}.json'.format(today), 'w', encoding='utf-8') as file:\n",
    "    json.dump(content_list, file, ensure_ascii= False)\n",
    "with open('ptt_comment{}.json'.format(today), 'w', encoding='utf-8') as file:\n",
    "    json.dump(comments_list, file, ensure_ascii= False)\n",
    "with open('ptt_log{}.json'.format(today), 'w', encoding='utf-8') as file:\n",
    "    json.dump(except_list, file, ensure_ascii= False)\n",
    "\n",
    "##############################################\n",
    "\n",
    "##讀取剛才爬好的檔案\n",
    "file_name = 'ptt_content{}.json'.format(today)\n",
    "ptt_today = pd.read_json(file_name, encoding = 'utf-8')\n",
    "\n",
    "#過濾掉標題有公告的列\n",
    "ptt_new = ptt_today[ptt_today['title'].str.contains('公告') != True]    \n",
    "    \n",
    "#將ptt_id有(台灣的分割)\n",
    "ip_cut = ptt_new['ip'].to_string()\n",
    "ip_adress = ip_cut.split()\n",
    "\n",
    "#將取好乾淨ptt_ip存回df\n",
    "ptt_new['ip'] = ip_adress[1:len(ip_adress):3]\n",
    "\n",
    "\n",
    "##############################################\n",
    "# Convert IP into city name\n",
    "# Base on the city name convert into North(1), Middel(2), South(3), East(4), and others(0) \n",
    "\n",
    "ip_list = list(ptt_new['ip'])\n",
    "\n",
    "print('Convert IP into city name. Starts...')\n",
    "\n",
    "city_list = []\n",
    "count = len(ip_list)\n",
    "start = time.time()\n",
    "#driver = webdriver.Chrome(ChromeDriverManager().install()) #For Mac OS users\n",
    "driver = webdriver.Chrome(executable_path=\"chromedriver.exe\") #For Windows users\n",
    "driver.get('https://www.ez2o.com/App/Net/IP')\n",
    "for ip_addr in ip_list:\n",
    "    print(count,'...')\n",
    "    count -= 1\n",
    "    elem = driver.find_element_by_xpath(\"//input[@id='QueryIP']\").clear()\n",
    "    elem = driver.find_element_by_xpath(\"//input[@id='QueryIP']\")\n",
    "    elem.send_keys(ip_addr) # ex: 218.173.71.162\n",
    "    elem = driver.find_element_by_xpath(\"//button[@class='btn btn-primary']\")\n",
    "    elem.click()\n",
    "    elem = driver.find_element_by_xpath(\"//tbody/tr[@class='active'][3]/td[2]\")\n",
    "    city_list.append(elem.text)\n",
    "driver.close()\n",
    "end = time.time()\n",
    "\n",
    "minute = round((end - start)/60)\n",
    "second = round((end - start)%60)\n",
    "\n",
    "ptt_new['city'] = city_list\n",
    "\n",
    "##############################################\n",
    "# Convert IP into city name\n",
    "# Base on the city name convert into North(1), Middel(2), South(3), East(4), and others(0) \n",
    "\n",
    "\n",
    "north = ['Keelung','Keelung City','Zhubei','Taipei','New Taipei City',' Taipei County','Taoyuan District','Hsinchu','Yilan County','Yilan']\n",
    "middle = ['Miaoli','Miaoli City','Yuanlin','Toufen Township','Taichung','Taichung City','Huwei','Nantou City','Puli Town','Douliu','Chang-hua','Yunlin County']\n",
    "south = ['Chiayi City','Tainan City','Kaohsiung City','Pingtung City']\n",
    "east = ['Hualien City','Hualien County','Taitung County','Taitung City']\n",
    "\n",
    "city_list = list(ptt_new['city'])\n",
    "\n",
    "area_code = []\n",
    "for city in city_list:\n",
    "    if city in north:\n",
    "        area_code.append(1)\n",
    "    elif city in middle:\n",
    "        area_code.append(2)\n",
    "    elif city in south:\n",
    "        area_code.append(3)\n",
    "    elif city in east:\n",
    "        area_code.append(4)\n",
    "    else:\n",
    "        area_code.append(0)\n",
    "\n",
    "ptt_new['area'] = area_code\n",
    "\n",
    "# with open('ptt_new_10page_content_final.json'.format(file_name), 'w', encoding='utf-8') as file: # Change the output file name\n",
    "#     ptt.to_json(file, force_ascii=False, orient='records')\n",
    "\n",
    "##############################################\n",
    "##將ptt_time分割\n",
    "#取week\n",
    "weekday_cut = ptt_new['time'].to_string()\n",
    "weekday = weekday_cut.split()\n",
    "#將取好乾淨ptt_week另存df欄位\n",
    "ptt_new['weekday'] = weekday[1:len(weekday):6]\n",
    "\n",
    "\n",
    "#取month\n",
    "month_cut = ptt_new['time'].to_string()\n",
    "month = month_cut.split()\n",
    "ptt_new['month'] = month[2:len(month):6]\n",
    "\n",
    "#取day\n",
    "day_cut = ptt_new['time'].to_string()\n",
    "day = day_cut.split()\n",
    "ptt_new['day'] = day[3:len(weekday):6]\n",
    "\n",
    "#取time\n",
    "time_cut = ptt_new['time'].to_string()\n",
    "time = time_cut.split()\n",
    "ptt_new['time2'] = time[4:len(weekday):6]\n",
    " \n",
    "#取year\n",
    "year_cut = ptt_new['time'].to_string()\n",
    "year = year_cut.split()\n",
    "ptt_new['year'] = weekday[5:len(weekday):6]\n",
    "\n",
    "##############################################\n",
    "##轉換weekday成數字\n",
    "ptt_new['weekday'].replace('Mon',1,inplace= True) #inplace = true改變原數據\n",
    "ptt_new['weekday'].replace('Tue',2,inplace= True)\n",
    "ptt_new['weekday'].replace('Wed',3,inplace= True)\n",
    "ptt_new['weekday'].replace('Thu',4,inplace= True)\n",
    "ptt_new['weekday'].replace('Fri',5,inplace= True)\n",
    "ptt_new['weekday'].replace('Sat',6,inplace= True)\n",
    "ptt_new['weekday'].replace('Sun',7,inplace= True)\n",
    "\n",
    "##轉換month成數字\n",
    "ptt_new['month'].replace('Jan',1,inplace= True) #inplace = true改變原數據\n",
    "ptt_new['month'].replace('Feb',2,inplace= True)\n",
    "ptt_new['month'].replace('Mar',3,inplace= True)\n",
    "ptt_new['month'].replace('Apr',4,inplace= True)\n",
    "ptt_new['month'].replace('May',5,inplace= True)\n",
    "ptt_new['month'].replace('Jun',6,inplace= True)\n",
    "ptt_new['month'].replace('Jul',7,inplace= True)\n",
    "ptt_new['month'].replace('Aug',8,inplace= True)\n",
    "ptt_new['month'].replace('Sep',9,inplace= True)\n",
    "ptt_new['month'].replace('Oct',10,inplace= True)\n",
    "ptt_new['month'].replace('Nov',11,inplace= True)\n",
    "ptt_new['month'].replace('Dec',12,inplace= True)\n",
    "\n",
    "##合併year,month,day欄位成date欄位\n",
    "year = ptt_new['year'].astype('str')\n",
    "month = ptt_new['month'].astype('str')\n",
    "day = ptt_new['day'].astype('str')\n",
    "ptt_new['date'] = year+'/'+month+'/'+day\n",
    "\n",
    "#drop time欄位\n",
    "ptt_new = ptt_new.drop(\"time\", axis = 1)\n",
    "\n",
    "#修改time2名稱成time\n",
    "ptt_new = ptt_new.rename(columns={'time2':'time'})\n",
    "\n",
    "##############################################\n",
    "# Insert into the database\n",
    "import mysql.connector\n",
    "import time\n",
    "cnx = mysql.connector.connect(user='june', password='june',\n",
    "                              host='192.168.35.119',\n",
    "                              database='ptt_db')\n",
    "cursor = cnx.cursor()\n",
    "query = (\"SELECT url FROM test_update;\")  # check the id list, and use it as the base to either UPDATE or INSERT \n",
    "cursor.execute(query)\n",
    "\n",
    "url_list =[]\n",
    "\n",
    "for i in cursor:\n",
    "    url_list.append(i[0])\n",
    "    \n",
    "\n",
    "for i in range(len(ptt_new)):\n",
    "    content_list = {'area': int(ptt_new.iloc[i]['area']),'city': str(ptt_new.iloc[i]['city']), \n",
    "                        'content': str(ptt_new.iloc[i]['content']),'time': str(ptt_new.iloc[i]['time']),'title': str(ptt_new.iloc[i]['title']),\n",
    "                        'day': str(ptt_new.iloc[i]['day']),'weekday': str(ptt_new.iloc[i]['weekday']),'year': str(ptt_new.iloc[i]['year']),\n",
    "                        'month': str(ptt_new.iloc[i]['month']),'date': str(ptt_new.iloc[i]['date']),'time': str(ptt_new.iloc[i]['time']),\n",
    "                        'push': int(ptt_new.iloc[i]['push']),'boo': int(ptt_new.iloc[i]['boo']),'total': int(ptt_new.iloc[i]['total']),'ip': str(ptt_new.iloc[i]['ip']),'url': str(ptt_new.iloc[i]['url'])}\n",
    "    if ptt_new.iloc[i]['url'] in url_list: # Update  \n",
    "        #Insert into Database\n",
    "        update_article = \"UPDATE test_update SET date = %(date)s, year = %(year)s, month = %(month)s, day = %(day)s , area = %(area)s, city = %(city)s , weekday = %(weekday)s, time = %(time)s, title = %(title)s ,push = %(push)s, boo = %(boo)s, total = %(total)s, ip = %(ip)s, content = %(content)s WHERE url = %(url)s\"\n",
    "        # Insert new article\n",
    "        cursor.execute(update_article,content_list)\n",
    "        # Make sure data is committed to the database\n",
    "        cnx.commit()\n",
    "        print(i,\":\",'Updated the database.')\n",
    "    else: # Insert    \n",
    "        #Insert into Database\n",
    "        add_article = (\"INSERT test_update \"\n",
    "                       \"(url,area,city,content,ip,date,time,title, weekday,year,month,day, push, boo, total) \"\n",
    "                       \"VALUES (%(url)s, %(area)s, %(city)s, %(content)s, %(ip)s , %(date)s ,%(time)s, %(title)s ,%(weekday)s,%(year)s,%(month)s,%(day)s,%(push)s,%(boo)s ,%(total)s);\")\n",
    "        # Insert new article\n",
    "        cursor.execute(add_article,content_list)\n",
    "        # Make sure data is committed to the database\n",
    "        cnx.commit()\n",
    "        print(i,\":\",'Inserted into the database.')\n",
    "        \n",
    "cursor.close()\n",
    "cnx.close()\n",
    "##############################################\n",
    "        \n",
    "end = time.time()\n",
    "minute = round((end - start) / 60)\n",
    "second = round((end - start) % 60)\n",
    "print('Finished')\n",
    "print('Total time:', minute, 'm', second, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "ptt_automiatic_update_final.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
